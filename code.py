# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BxIblaof4Ea4Qs88Gqnn8ZftjJu-_PrX
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import VGG16
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

# Define the path to the main dataset directory
dataset_dir = '/content/drive/MyDrive/Grapes Dataset'

# Define image dimensions and batch size
img_width, img_height = 224, 224
batch_size = 32

# Create an ImageDataGenerator for data augmentation and preprocessing
datagen = ImageDataGenerator(
    rescale=1./255,            # Rescale pixel values to [0, 1]
    rotation_range=40,         # Increase rotation range for more variations
    width_shift_range=0.3,     # Increase width shift range
    height_shift_range=0.3,    # Increase height shift range
    shear_range=0.3,           # Increase shear range
    zoom_range=0.3,            # Increase zoom range
    horizontal_flip=True,      # Randomly flip images horizontally
    fill_mode='nearest',       # Fill mode for new pixels created during augmentations
    validation_split=0.2       # Split the data into training and validation sets
)

# Generate batches of training data with augmentation from the 'grapes_dataset' directory
train_generator = datagen.flow_from_directory(
    dataset_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='categorical',  # Use 'categorical' for multi-class classification
    shuffle=True,              # Shuffle the data for training
    subset='training'          # Set as training data
)

# Generate batches of validation data from the 'grapes_dataset' directory
validation_generator = datagen.flow_from_directory(
    dataset_dir,
    target_size=(img_width, img_height),
    batch_size=batch_size,
    class_mode='categorical',  # Use 'categorical' for multi-class classification
    shuffle=False,             # Do not shuffle the data for validation
    subset='validation'        # Set as validation data
)

# Load the VGG16 model pre-trained on ImageNet, excluding the top layers
base_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))

# Freeze the base model to prevent updating its pre-trained weights during training
base_model.trainable = False

# Create a new model on top of the VGG16 base
model = Sequential([
    base_model,
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(train_generator.num_classes, activation='softmax')
])

# Compile the model
model.compile(optimizer=Adam(learning_rate=1e-4),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Print model summary
model.summary()

# Define number of training steps per epoch and validation steps
train_steps_per_epoch = train_generator.samples // batch_size
validation_steps = validation_generator.samples // batch_size

# Train the model
history = model.fit(
    train_generator,
    steps_per_epoch=train_steps_per_epoch,
    epochs=30,  # Increase epochs for more training iterations
    validation_data=validation_generator,
    validation_steps=validation_steps
)

# Plot training history (accuracy and loss)
def plot_training_history(history):
    plt.figure(figsize=(10, 5))

    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Train Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Train Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title('Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.show()

# Plot training history
plot_training_history(history)

# Evaluate the model on the validation dataset
validation_loss, validation_accuracy = model.evaluate(validation_generator, steps=validation_steps)
print(f'Validation Accuracy: {validation_accuracy:.4f}')

# Function to make predictions on a single image
def predict_image(image_path):
    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(img_width, img_height))
    img_array = tf.keras.preprocessing.image.img_to_array(img)
    img_array = tf.expand_dims(img_array, 0) / 255.0

    prediction = model.predict(img_array)
    predicted_class_idx = np.argmax(prediction, axis=1)
    class_map = train_generator.class_indices
    predicted_class_label = list(class_map.keys())[predicted_class_idx[0]]

    plt.imshow(img)
    plt.axis('off')
    plt.title(f'Predicted Class: {predicted_class_label}')
    plt.show()

# Example usage: Provide path to a new image for prediction
new_image_path = '/content/download (1).jpeg'
predict_image(new_image_path)